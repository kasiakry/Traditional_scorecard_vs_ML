---
title: Comparison of models for credit risk purposes - logistic regression vs random
  forest
author: "Katarzyna Kry≈Ñska"
date: "5 07 2020"
output: 
  rmdformats::material:
    highlight: tango
    cards: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(rmdformats)
library(corrplot)
library(RColorBrewer)
library(kableExtra)
library(tidyverse)
library(stargazer)
library(stats)
library(pROC)
library(caret)
```

<style>

table, td, th {
  border: none;
  padding-left: 1em;
  padding-right: 1em;
  margin-left: auto;
  margin-right: auto;
  margin-top: 1em;
  margin-bottom: 1em;
}

</style>

```{r libraries outside_mrkdown, include=F, eval=F}
source("ivmult.r")
library(ggplot2)
library(smbinning)
library(forcats)
library(pROC)
library(stringr)
```

```{r do_not_run, eval=F, include=F}
# load data from OpenML and save to to a local directory
library(OpenML)
credit_OpenML <- getOMLDataSet(data.id = 31)
df <- credit_OpenML$data
save(df, file="df.rda")
```


# Abstract

The main of this research is to compare traditional logistic regression with random forest for the purpose of scorecard. This research will be based on empirical results only. Firstly, we will estimate a logistic regression model, in accordance with the best practices. Then we will train a Random Forest Classifier. Finally, we will compare the predictions of both approaches, using various performance measures.

# Dataset description

```{r load_df, eval=F, include=F}
load("load/df.rda")

# all variables are either factor or numeric, so that's good
# BUT class has levels 1-2 and not 0-1, which might be a problem?
# safer to change it into 0-1 values, where 0 is good and 1 is bad
df$class <- as.numeric(df$class)-1
```

```{r summary_df, eval=F, include=F}
# PRE-ANALYSIS OF DATA - SUMMARY, DESCRIPTION

## summary
#summary(df)

## Desc
#library(DescTools)
#Desc(df)

## are there any NAs? ----> NO
colSums(is.na(df))

## str
str(df)

# df$class <- factor(as.numeric(df$class) - 1, levels = c(0,1), labels = c("good", "bad"))

```

```{r merge_factor_variables, include=F, eval=F}

## Merge some factor variables

# credit_history
# merge "all paid" with "no credits/all paid"
new_labels <- levels(df$credit_history)[!levels(df$credit_history) %in% c("all paid")]
new_levels <- c(1, 3, 4, 5)
df$credit_history <- factor(ifelse(as.numeric(df$credit_history) %in% c(1,2), 1, df$credit_history), labels=new_labels)

# purpose
# 6 "repairs" dorzucam do 11 "other"
# 5 "domestic appliance" do 3 "furniture/equipment"
# 9 "retraining" do 7 "education"

new_levels <- c(1:11)[!c(1:11) %in% c(6, 5, 9)]
new_labels <- levels(df$purpose)[new_levels]
new_labels[5] <- "education/retraining"

df$purpose <- as.numeric(df$purpose)
df$purpose <- ifelse(df$purpose %in% c(6,11), 11, df$purpose)
df$purpose <- ifelse(df$purpose %in% c(5,3), 3, df$purpose)
df$purpose <- ifelse(df$purpose %in% c(9,7), 7, df$purpose)
df$purpose <- factor(df$purpose, levels=new_levels, labels=new_labels)

# existing_credits
df$existing_credits <- ifelse(df$existing_credits %in% c(3,4), 3, df$existing_credits)
new_levels <- c(1:3)
new_labels <- c("1", "2", ">2")
df$existing_credits <- factor(df$existing_credits, levels=new_levels, labels=new_labels)

```

```{r DISCRETIZATION, eval=F, include=F}
# DISCRETIZATION - FINE CLASSING

## division variables into FACTORS and NUMERIC variables
base <- df[, colnames(df) != "class"]
base.n<-base[,sapply(base, is.numeric)]
base.f<-base[,sapply(base, is.factor)]

## percentiles
percentile<-apply(X=base.n, MARGIN=2, FUN=function(x) round(quantile(x, seq(0.1,1,0.1), na.rm=TRUE),2))

## unique values per column
unique<-apply(base.n, MARGIN=2, function(x) length(unique(x)))

## selecting only columns with more than 10 unique levels
## binarization for numeric variables with >10 unique values
numeric<-colnames(base.n[which(unique>=10)])
for (m in numeric){
  base.n[,paste(m,"_fine", sep="")]<-cut(
    x=as.matrix(base.n[m]), 
    breaks=c(-Inf,unique(percentile[,m])), 
    labels =c(paste("<=",unique(percentile[,m])))
    ) 
}

## columns with <10 levels will be transformed to factors
num_as_fact<-colnames(base.n[which(unique<10 & unique>1 )])
base.f[,num_as_fact]<-lapply(base.n[,num_as_fact],as.factor)
base.n <- base.n[,!names(base.n) %in% num_as_fact]
```

```{r WOE_calc, eval=F, include=F}
# Calculation of Weight of Evidence (WoE) using smbinning

## in smbinning 0 means bad, --> reverting definition of gb flag
base.n$class_woe<-(1-df$class)
base.n$class <- df$class

base.f$class_woe <- base.n$class_woe
base.f$class <- base.n$class

## Choice of vars to be analysed
names.n<-colnames(base.n[,!names(base.n) %in% c(
  colnames(base.n[ ,grepl(pattern="_fine" , x=names(base.n))]),
  "class",
  "class_woe")])
names.f<-colnames(base.f[,!names(base.f) %in% c("class","class_woe")])

## List for WOE - preallocation
WOE <- list()

## Dataframe for Information Value
IV <- data.frame(VAR=character(), IV=integer())
```

```{r WoE_numeric.pdf, eval=F, include=F}
# ANALYSIS OF NUMERIC VARIABLES

# Storing charts in a pdf file
pdf(file="WoE_numeric.pdf",paper="a4")


#Set up progress bar
pb<- txtProgressBar(min = 0, max = length(names.n), style = 3)

for (i in names.n){
  
  # rows and columns at a chart
  par(mfrow=c(2,2))
  
  # smbinning.custom(df, y, x, cuts)
  # df - data frame
  # y - GB flag
  # x - risk factors
  # cuts - cut-offs
  results<- smbinning.custom(df=base.n, y="class_woe", x=i, cuts=unique(percentile[,i]))
  
  # BOXPLOT
  boxplot(base.n[,i]~base.n$class, horizontal=T, frame=F, col="lightgray",main="Distribution") 
  mtext(i,3) 
  # Frequency plot 
  smbinning.plot(results,option="dist",sub=i)
  # Bad rate fractions
  smbinning.plot(results,option="badrate",sub=i) 
  # WoE
  smbinning.plot(results,option="WoE",sub=i)

  # IV row binding
  IV<-rbind(IV, as.data.frame(cbind("VAR"=i, "IV"=results$ivtable[results$ivtable$Cutpoint=="Total", "IV"])))
  
  # Saving data 
  d<-results$ivtable[,c("Cutpoint","WoE","PctRec")]
  # Total row removal  
  d<-d[d$Cutpoint!="Total",]
  # Ordering wrt WoE - for gini etc. calculation
  d<-d[with(d, order(d$WoE)),]
  # Id 
  d$numer<-11:(nrow(d)+10)
  # Saving WoE in list 
  WOE[[i]]<-d
  
  #Update progess bar  
  setTxtProgressBar(pb,  min(grep(i, names.n)))
} 

close(pb)
dev.off()

# Reseting charts options
resetPar <- function() {
  dev.new()
  op <- par(no.readonly = TRUE)
  dev.off()
  op
}

resetPar()

# Insights:
# 1) duration - almost linear relationship between credit duration and probability of default
# 2) credit_amount - looks like a non-linear relationship, mid credit_amounts have the smallest pod,
# while the smallest and especially biggest credits are more prone to default
# 3) age - relationship not clear, the risk seems to be dropping with bigger age, but people (52;75] seem
# riskier than people (36;52]
```

```{r WoE_factor.pdf, include=F, eval=F}
str(base.f)

# PDF
pdf(file="WoE_factor.pdf",paper="a4")

# progress bar
pb<- txtProgressBar(min = 0, max = length(names.f), style = 3)

for (i in names.f){

  #neede to join results for factors and numerics
  base.f[,paste(i,"_fine", sep="")]<-base.f[,i]
    
  par(mfrow=c(2,2))
  
  results <- smbinning.factor(df=base.f, y="class_woe", x=i, maxcat=length(unique(base.f[,i])))
  # df - data frame
  # y - GB
  # x - variable 
  # maxcat - maximal accepted number of categories

  smbinning.plot(results,option="dist",sub=i) 
  smbinning.plot(results,option="badrate",sub=i) 
  smbinning.plot(results,option="WoE",sub=i)
  
  IV<-rbind(IV, as.data.frame(cbind("VAR"=i, "IV"=results$ivtable[results$ivtable$Cutpoint=="Total", "IV"])))

  d<-results$ivtable[,c("Cutpoint","WoE","PctRec")]
 
  d<-d[d$Cutpoint!="Total",]
 
  d<-d[with(d, order(d$WoE)),]
  
  d$numer<-11:(nrow(d)+10)
  
  WOE[[i]]<-d
  
  setTxtProgressBar(pb,  min(grep(i, names.f)))
} 
close(pb)
dev.off()

```

```{r variables_quality_gini_iv, eval=F, include=F}
base <- cbind(base.f[,!names(base.f) %in% c("class_woe", "class")], base.n)

# progress bar
pb<- txtProgressBar(min = 0, max = length(names(WOE)), style = 3)
stats<-cbind(IV, Gini=NA, miss=NA)
l<-"checking_status"

for (l in names(WOE)){

  # objects need for Gini calculation
  variable<-base[,c("class_woe", paste(l,"_fine", sep=""))]
  woe<- WOE[[l]][c("Cutpoint", "WoE")]
  
  # Cutpoint values change if Factor
  if (is.character(woe$Cutpoint)==TRUE) { 
    woe$Cutpoint<-as.factor(gsub("= '|'", "", woe$Cutpoint))
    woe$Cutpoint<-as.factor(woe$Cutpoint)
  }
  
  # Var levels and WoE merging
    # merge()
    # x,y - tables
    # by.x .y key
    # all.x=T - left join
  
  dat_temp<-merge(variable, woe, by.x=paste(l,"_fine", sep=""), by.y="Cutpoint", all.x=T)

 # name change
  colnames(dat_temp)[which(names(dat_temp) == "WoE")] <- paste(l, "_woe", sep="")  
  
  # adding WoE var to original dataset
  base<-merge(base, woe, by.x=paste(l,"_fine", sep=""), by.y="Cutpoint", all.x=T)
  colnames(base)[which(names(base) == "WoE")] <- paste(l, "_woe", sep="")
  
  # gini claculation
  #   gini = 2*AUROC-1
  #   auc() function for AUROC calculation
  #   ?pROC
  
  gini<- c(2*auc(dat_temp$class_woe,dat_temp[,paste(l, "_woe", sep="") ])-1)
  
  stats[stats$VAR==l, "Gini"]<-gini
  
  miss<-1-c(nrow(dat_temp[dat_temp[,paste(l,"_fine", sep="")]!='Missing', ])/nrow(dat_temp))
  
  stats[stats$VAR==l, "miss"]<-miss
  
  setTxtProgressBar(pb,  min(grep(l, names(WOE))))
  
}

close(pb)

write.csv(stats, "stats.csv")

```

```{r variables_quality_kendall_correlation, include=F, eval=F}

# only _woe variables
var_to_check<-colnames(base)[grep("_woe", colnames(base))]
# exclusion of class_woe
var_to_check<-var_to_check[-1]
base_kor<-base[,var_to_check]

# cor()
# x- base
# method - type of corr coeff
# use - what to do if missings?; "pairwise" - deleting observations only if missing in considered vars,
kendall <-cor(base_kor, method = "kendall",use="pairwise")
save(kendall, file="load/kendall.Rdata")
```

```{r variables_selection, include=F, eval=F}

corr_list<-data.frame()

# ordering correlation matrix wrt Gini (rows & columns)
#woe var creation
stats$VARW<-paste(stats$VAR,"_woe",sep="")
# merging statistics & correlation results
stat<-merge(x = stats, y = kendall, by.x = "VARW",by.y="row.names",all.y=T)
#ordering wrt Gini - descending
stat<-stat[ order(-stat[,"Gini"]), ]
#changing row names as variables names
row.names(stat)<-stat[,1]
#only correlation matrix
stat<-stat[6:length(stat[1,])]
#vector of names
cols<-row.names(stat[,])
#coloum ordering wrt Gini
stat<-stat[cols]

#if no correlation (missings) then small value of correlation assigning, 
#that won't delete var from the analysis
temp_k<-stat
temp_k<-replace(temp_k, is.na(temp_k), 0.000001000)
# 
# loop reapet()
# it works till stop

library(plyr)
threshold<-0.5

repeat{
  #even though this condistion is at the beggining, it has rather final character 
  # condition to stop a loop is reduction to NULL object
  # it may happend when in the last loop all variables satisfy a condition
  # > threshold 
  if (length(temp_k)<1) {
    break
  }
  
  # selection of all variables that correlation with a first variable is higher than threshold
  # NOTE: correlation coefficient between variable and  itself is equal 1 so it wuold be taken into data frame
  row_k<-abs(temp_k[1,]) > threshold
  row_k[1]<-TRUE
  
  # condition checkig wheter we are at the end of the analysis
  # if not then vars that > threshold is satisfied are taken
  # in yes the final variable is taken
  if(length(row_k)>1){
    row_k2<-row_k[,row_k]
  }else{row_k2<-row_k}
  
  # from temp_k all rows kept in row_k2 are deleted
  
  temp_k<-as.data.frame(temp_k[!t(row_k),!row_k])
  
  #if only one variable remains then R would reduce DF to vector and names for rows and columns have to be defined
  if(length(temp_k)==1){ 
    colnames(temp_k)<- dimnames(row_k)[[2]][-which(row_k)]
    rownames(temp_k)<- dimnames(row_k)[[2]][-which(row_k)]
  }
  
  
  #creation of vecotr with variables that satisfy > threshold
  if(length(row_k2)==1) {
    variable<-row.names(row_k)
    namess<-as.data.frame(variable)
  }
  
  if(length(row_k2)>1) {
    row_k2<-row_k2[2:length(row_k2)]
    row_k2<-as.data.frame(t(row_k2))
    namess<-colnames(row_k2)
    namess<-as.data.frame(t(namess))
    row.names(namess) <- row.names(row_k)
    variable<-row.names(row_k)
    variable<-as.data.frame(variable)
    namess<-merge(variable,namess)
  }
  
  
  # adding to corr_list row with all correlated vars with the variable
  # rbind.fill() - filling missing columns with NA
  
  corr_list<-rbind.fill(corr_list,namess)
  
  # condition to leave a loop - when last variable left
  
  if(length(temp_k)==1){ 
    variable<-dimnames(row_k)[[2]][-which(row_k)]
    namess<-as.data.frame(variable)
    corr_list<-rbind.fill(corr_list,namess)
    break}
  
}

# merging of data with stats and correlations
all<-merge(x = stats, y = corr_list, by.x = "VARW",by.y="variable",all.y=T)
# ordering by Gini
all<-all[ order(-all[,"Gini"]), ]
# exclusion of variables with gini below 0.1
all<-all[all$Gini>0.1,]

base_coarse<-base[-grep("_fine",colnames(base))]
kols<-c(as.character(all$VAR),"class","class_woe")
base_coarse<-base_coarse[,kols]

```

```{r sample_split, include=F, eval=F}

#drawing with stratification  
library(caTools)

# two subsamples, one for each subsample
# list with number of strata dimensions
# f - strata
d_split<-split(x=base_coarse, f=base_coarse$class)

# table train i test
train = list()
test=list()


# for loop for each strata

for (i in 1:length(d_split)){
  
  set.seed(1916)

  # splitting vector for i-th strata
  assign(
    paste("sample",i,sep="_"),sample.split(d_split[[i]][,1], SplitRatio = .7))
  
  # training set for i-th starta
  assign(
    paste("train",i,sep="_"),subset(d_split[[i]],  get(paste("sample",i,sep="_")) == TRUE))
  
  # validation set for i-th starta
  assign(
    paste("test",i,sep="_"),subset(d_split[[i]], get(paste("sample",i,sep="_")) == FALSE))
  
  train[[i]]<-get(paste("train",i,sep="_"))
  test[[i]]<-get(paste("test",i,sep="_"))
}

# row binding of lists train & test
train = do.call(rbind, train)
test = do.call(rbind, test)

# removing unnecessery objects
rm( test_1, test_2
    , train_1, train_2
    , sample_1, sample_2)

mean(train$class)
mean(test$class)

save(train,file="train.RData")
save(test,file="test.RData")

```

```{r WoE_coarse_numeric.pdf, include=F, eval=F}
load("load/test.RData")
load("load/train.RData")

columns <- colnames(train)[-which(names(train) %in% c("class","class_woe"))]
columns.n<-columns[sapply(train[,columns], is.numeric)]

# progress bar
waitBar <- txtProgressBar(min = 0, max = length(columns.n), style = 3)

temp<-data.frame(VAR=character(), VALUE=integer())
stats<-data.frame(VAR=character(), IV=integer(),Gini=integer(), MISS=integer(),IVw=integer(),Giniw=integer(), MISSw=integer())

#cut offs for testing data
cut_offs<-data.frame(VAR=character(), cuts=integer())

current_var<-columns.n[2]

pdf("WoE_coarse_numeric.pdf")

for (current_var in columns.n){
  
  par(xpd = T, mar = par()$mar, mfrow=c(2,2))
  
  # smbinning using decision tree
  # Optimal Binning categorizes a numeric characteristic into bins for ulterior usage in scoring modeling. 
  # This process, also known as supervised discretization, utilizes Recursive Partitioning to categorize the numeric characteristic.
  # The especific algorithm is Conditional Inference Trees which initially excludes missing values (NA) to compute the cutpoints, adding them back later in the process for the calculation of the Information Value.
  
  result<-smbinning(train[,c("class_woe", current_var)], 
                    y="class_woe", 
                    x=current_var, 
                    p=0.05)
  # for age
  if(length(result)<=1){
    # only for age
    age_cut <- c(25, 33)
    result<- smbinning.custom(df=train, y="class_woe", x=current_var, cuts=age_cut)
  }
  
  
  if(length(result)>1){
    points<-list(result$cuts)
    cut_offs<-rbind(cut_offs,as.data.frame(cbind("VAR"=current_var,"cuts"=points)))
    
    IV<-result$ivtable
    
    #coarsed variables  
    train[,paste(current_var, "_coarse", sep="")]<- cut(train[,current_var], 
                                                        breaks=c(-Inf,unique(result$cuts),Inf), 
                                                        labels=c(paste("<=", unique(result$cuts)),"<= Inf"),
                                                        include.lowest = T)
    train[,paste(current_var, "_coarse", sep="")]<-fct_explicit_na(train[,paste(current_var, "_coarse", sep="")], na_level="Missing")
    
    test[,paste(current_var, "_coarse", sep="")]<- cut(test[,current_var],
                                                       breaks=c(-Inf,unique(result$cuts),Inf), 
                                                       labels=c(paste("<=", unique(result$cuts)),"<= Inf"),
                                                       include.lowest = T)
    test[,paste(current_var, "_coarse", sep="")]<-fct_explicit_na(test[,paste(current_var, "_coarse", sep="")], na_level="Missing")
    
    
    IVw<-sum(iv.mult(test,"class",vars=paste(current_var,"_coarse",sep=""))[[1]][,"miv"])
    IV$Cutpoint<-ifelse(grepl(">",IV$Cutpoint)==T,"<= Inf",IV$Cutpoint)
    
    #addig WoE variable
    train<-merge(train,IV[,c("Cutpoint", "WoE")],by.x= paste(current_var, "_coarse", sep=""), by.y="Cutpoint", all.x=T, sort=F)
    colnames(train)[which(names(train) == "WoE")] <- paste(current_var, "_woe", sep="") 
    
    test<-merge(test,IV[,c("Cutpoint", "WoE")],by.x= paste(current_var, "_coarse", sep=""), by.y="Cutpoint", all.x=T, sort=F)
    colnames(test)[which(names(test) == "WoE")] <- paste(current_var, "_woe", sep="")
    
    #GINI
    gini<- 2*auc(train$class_woe,train[,paste(current_var, "_woe", sep="") ])-1
    giniw<- 2*auc(test$class_woe,test[,paste(current_var, "_woe", sep="") ])-1
    # % missings
    miss<-1-nrow(train[!(is.na(train[,current_var])),])/nrow(train)
    missw<-1-nrow(test[!(is.na(test[,current_var])),])/nrow(test)
    #stats<-rbind(stats, as.data.frame(cbind("VAR"=current_var, "IV"=IV[IV$Cutpoint=="Total", "IV"],"Gini"=gini,  "MISS"=miss,"IVw"=IVw, "Giniw"=giniw,  "MISSw"=missw)))
    
    plot<-IV[!is.na(IV$WoE) & IV$Cutpoint!="Total",]
    plot$WoE<-ifelse(plot$WoE==Inf, 8, ifelse(plot$WoE==-Inf, -8,plot$WoE))
    g<-barplot(plot$WoE,names.arg=plot$Cutpoint, cex.names=0.5, main=current_var, xaxt="n")
    print(g)
    axis(1,g,plot$Cutpoint, tick=F, las=2, cex.axis=0.7)
    text(g,plot$WoE, labels=paste("WoE=",format(plot$WoE,digits=1)," \n Fill=",round(plot$PctRec,3)*100, "%", sep=""), xpd = T, col = "black", pos=3, cex=0.7)
    
    # stability
    freq.test<-as.data.frame(table(test[,paste(current_var, "_coarse", sep="")])/ nrow(test))
    colnames(freq.test)<-c("Cutpoint", "PctRec")
    freq.train<-plot[,c("Cutpoint","PctRec")]
    freq<-merge(freq.train,freq.test, all.x=T, by="Cutpoint")
    colnames(freq)<- c("Cutpoint","Train","Test")
    rownames(freq)<-freq[,1]
    freq<-freq[,-1]
    
    stacked<-barplot(as.matrix(freq), beside=F, main=current_var)
    print(stacked)
    legend("topright", legend=row.names(freq), cex=0.6, bty="o",pch=21)
    
    smbinning.plot(result,option="dist",sub=current_var)
    
    smbinning.plot(result,option="badrate",sub=current_var) 
    
  } 
  
  setTxtProgressBar(waitBar,  min(grep(current_var, columns.n)))
}
dev.off()

```

```{r woeBinning, include=F, eval=F}
library(woeBinning)
columns.f <- columns[sapply(train[,columns], is.factor)]
woe.bin_f <- woe.binning(train, 
                         target.var="class", 
                         pred.var=columns.f, 
                         min.perc.total=0.05,
                         min.perc.class=0.02, 
                         stop.limit=0.1)
# min.perc.total =	
#   For numeric variables this parameter defines the number of initial classes before any merging is applied. For example min.perc.total=0.05 (5%) will result in 20 initial classes. 
#   For factors the original levels with a percentage below this limit are collected in a ‚Äòmiscellaneous‚Äô level before the merging based on the min.perc.class and on the WOE starts. 
#   Accepted range: 0.01-0.2; default: 0.05.

# min.perc.class =
#   If a column percentage of one of the target classes within a bin is below this limit (e.g. below 0.01=1%) then the respective bin will be joined with others. In case of numeric variables adjacent predictor classes are merged. 
#   For factors respective levels (including sparse NAs) are assigned to a ‚Äòmiscellaneous‚Äô level. 
#   Accepted range: 0-0.2; default: 0, i.e. no merging with respect to sparse target classes is applied.

# stop.limit	
# Stops WOE based merging of the predictor's classes/levels in case the resulting information value (IV) decreases more than x% (e.g. 0.05 = 5%) compared to the preceding binning step. stop.limit=0 will skip any WOE based merging. 
# Increasing the stop.limit will simplify the binning solution and may avoid overfitting. 
# Accepted range: 0-0.5; default: 0.1.

woe.binning.plot(woe.bin_f, multiple.plots=F)
woe.binning.table(woe.bin_f)
train<-woe.binning.deploy(train, woe.bin_f,add.woe.or.dum.var='woe')
test<-woe.binning.deploy(test, woe.bin_f,add.woe.or.dum.var='woe')


# I should unify the names of binned/coarse variables and woe

## woe.credit_history.binned --------> credit_history_woe
pattern <- "woe.\\w+.binned"
idx <- str_detect(colnames(train), regex(pattern))
new_str <- str_extract(colnames(train),pattern) %>% str_remove("woe.") %>% str_remove(".binned") %>% str_c("_woe")
colnames(train) <- ifelse(idx, new_str, colnames(train))
colnames(test) <- ifelse(idx, new_str, colnames(train))
## checking_status.binned ----------> "checking_status_coarse
pattern <- "\\w+.binned"
idx <- str_detect(colnames(train), regex(pattern))
new_str <- str_extract(colnames(train),pattern) %>% str_remove(".binned") %>% str_c("_coarse")
colnames(train) <- ifelse(idx, new_str, colnames(train))
colnames(test) <- ifelse(idx, new_str, colnames(train))

str(test)

```

```{r WOE_coarse_all.pdf, include=F, eval=F}

columns.coarse <- str_extract(colnames(train),"\\w+_coarse")[str_detect(colnames(train), "\\w+_coarse")]

# progress bar
waitBar<- txtProgressBar(min = 0, max = length(columns.coarse), style = 3)

resetPar <- function() {
  dev.new()
  op <- par(no.readonly = TRUE)
  dev.off()
  op
}

pdf("WOE_coarse_all.pdf")

current_variable<-columns.coarse[1]

for (current_variable in columns.coarse){
  
  par(xpd = T, mfrow=c(2,2))
  result<-smbinning.factor(train[,c("class_woe", current_variable)], y="class_woe", x=current_variable)

  points<-list(result$cuts)
  cut_offs<-rbind(cut_offs,as.data.frame(cbind("VAR"=current_variable,"cuts"=points)))
  
  IV<-result$ivtable
  IVw<-sum(iv.mult(test,"class",vars=current_variable)[[1]][,"miv"])
  IV$Cutpoint<-as.factor(gsub("= |'","",IV$Cutpoint))

  #GINI
  gini<- 2*auc(train$class_woe,train[,gsub("_coarse","_woe",current_variable) ])-1
  giniw<- 2*auc(test$class_woe,test[,gsub("_coarse","_woe",current_variable)])-1
  
  miss<-1-nrow(train[!(is.na(train[,current_variable])),])/nrow(train)
  missw<-1-nrow(test[!(is.na(test[,current_variable])),])/nrow(test)
  stats<-rbind(stats, as.data.frame(cbind("VAR"=current_variable, "IV"=IV[IV$Cutpoint=="Total", "IV"],"Gini"=gini,  "MISS"=miss,"IVw"=IVw, "Giniw"=giniw,  "MISSw"=missw)))
  
  #plot WoE
  plot<-IV[!is.na(IV$WoE) & IV$Cutpoint!="Total",]
  plot$WoE<-ifelse(plot$WoE==Inf, 8, ifelse(plot$WoE==-Inf, -8,plot$WoE))
  g<-barplot(plot$WoE,names.arg=plot$Cutpoint, cex.names=0.5, main=current_variable, xaxt="n")
  print(g)
  axis(1,g,plot$Cutpoint, tick=F, las=2, cex.axis=0.7)
  text(g,plot$WoE, labels=paste("WoE=",format(plot$WoE,digits=1)," \n Fill=",round(plot$PctRec,3)*100, "%", sep=""), xpd = T, col = "black", pos=3, cex=0.7)
  
  # stability
  
  freq.test<-as.data.frame(table(test[,current_variable])/ nrow(test))
  colnames(freq.test)<-c("Cutpoint", "PctRec")
  freq.test$Cutpoint <- str_replace(freq.test$Cutpoint,"<= ","<")
  freq.train<-plot[,c("Cutpoint","PctRec")]
  freq<-merge(freq.train,freq.test, all.x=T, by="Cutpoint")
  colnames(freq)<- c("Cutpoint","Train","Test")
  rownames(freq)<-freq[,1]
  freq<-freq[,-1]
  
  stacked<-barplot(as.matrix(freq), beside=F, main=current_variable)
  print(stacked)
  
  smbinning.plot(result,option="dist",sub=current_variable)
  
  smbinning.plot(result,option="badrate",sub=current_variable) 

setTxtProgressBar(waitBar,  min(grep(current_variable, columns.f)))
}

dev.off()
warnings()
par(resetPar()) 

#Saving -----------------------------------

#write.csv(stats, file="stats_final.csv")

# change order of columns

train <- train %>% select(sort(current_vars()))
new_order <- c(colnames(train)[!colnames(train) %in% c("class", "class_woe")],
               "class",
               "class_woe")
train <- train %>% select(new_order)
test <- test %>% select(new_order)

str(train)
save(train,file="load/train_final.Rdata")
save(test,file="load/test_final.Rdata")
```

Dataset that was used in this research is a well-known *credit-g* dataset, available at OpenML [https://www.openml.org/d/31](https://www.openml.org/d/31). Original dataset contains 1 target value, *class*, which describes if the customer is good or bad, and 21 attributes:

1. *checking_status* - Status of existing checking account, in Deutsche Mark.
2. *duration* - Duration in months
3. *credit_history* - Credit history (credits taken, paid back duly, delays, critical accounts)
4. *purpose* - Purpose of the credit (car, television,...)
5. *credit_amount* - Credit amount
6. *savings_status* - Status of savings account/bonds, in Deutsche Mark.
7. *employment* - Present employment, in number of years.
8. *installment_commitment* - Installment rate in percentage of disposable income
9. *personal_status* - Personal status (married, single,...) and sex
10. *other_parties* - Other debtors / guarantors
11. *residence_since* - Present residence since X years
12. *property_magnitude* - Property (e.g. real estate)
13. *age* - Age in years
14. *other_payment_plans* - Other installment plans (banks, stores)
15. *housing* - Housing (rent, own,...)
16. *existing_credits* - Number of existing credits at this bank
17. *job* - Job
18. *num_dependents* - Number of people being liable to provide maintenance for
19. *own_telephone* - Telephone (yes,no)
20. *foreign_worker* - Foreign worker (yes,no)

The datasets contains data for 1000 customers, of which 700 are good and 300 are bad ones. This means that the data is not much imbalanced and there is enough number of bad clients to estimate/train a reliable model.

To build a scorecard model based on logistic regression, coarse classing of the data is needed. Firstly, continuous variables were binned into factor variables. The initial division into bins was made with respect to percentiles. In case of some variables which were originally factors, merging similar categories was done. 

In the end, we ended up with 20 factor variables. Then, we checked their correlation, using Kendall correlation measure. This is important in order not to introduce colinearity into our model. The result is plotted below:

```{r plot_kendall_corr, out.width = '80%', fig.align='center', fig.asp = .80}

load("load/kendall.Rdata")
corrplot(kendall, type="upper", 
         order="hclust",
         col=brewer.pal(n=8, name="BrBG"),
         tl.cex=0.55,
         tl.col="brown",
         tl.offset=1,
         tl.srt=45,)

```

It seems like there will not be any problems with colinearity in the model. We might only consider removing either *credit_history* or *existing_credits*, as their correlation is around 50%.

Then, we calculated some variables quality statistics, like Gini and Information Value.

```{r print_variables_quality_table}
quality_table <- read.csv("stats.csv")
quality_table <- quality_table %>% arrange(desc(Gini)) %>% subset(select=-c(X, miss))
quality_table$"No." <- c(1:20)
quality_table <- quality_table[c(4, 1, 2, 3)]
kable(quality_table, digits = 4) %>% kable_styling(bootstrap_options = c("striped", "hover","condensed", "responsive"))
```

We decided to include in our final dataset only these variables, for which Gini value is bigger than 10%. In the end we ended up with 11 explanatory variables.

To divide the dataset into *train* and *test* samples, we used stratified sampling. This way the ratio between number of good and bad clients is the same for *train* and *test* samples. 

Then, using *train* sample, we performed supervised discretization. We used Optimal Binning to categorise numeric variables into bins for scoring modeling. Only for age bins were not assigned by the algorithm, so they were created manually, based on WoE of previously created percentiles. In case of factor variables, they were automatically merged into groups with the use of woeBinning package. 

Charts for all analysed variables:

1. WoE for final bins,

2. Stability analysis - percentage of customers in each bin across train and test samples,

3. Distribution (percentage) of all customers in each bin,

4. Bad rate for each bin,

can be find in the pdf file on [Github](https://github.com/kasiakry/Traditional_scorecard_vs_ML/blob/master/WOE_coarse_all.pdf).

# Logistic regression

```{r base_model, include=F, eval=T}
load("load/test_final.Rdata")
load("load/train_final.Rdata")

source("quality_assessment.R")

cols <- colnames(train)[grep("woe", colnames(train))]
cols <-cols[-grep("class_woe", cols)]

df <- train[,c("class",cols)]

# model with constant only
base_model <- glm(class ~ 1,data=df, family=binomial("logit"))
summary(base_model)
# save(base_model,file="load/glm_base_model.Rdata")

# theoretical value  = ln(p/(1-p)) -> p=e^wd/(1+e^wd) -> e^-1.259/(1+e^-1.259) -> 22,11%
# constant -> expected value for a base group -> whole sample
exp(base_model$coefficients) / (1+exp(base_model$coefficients))
mean(train$class)

# model with all variables
max_model<-glm(class ~ .,data=df, family=binomial("logit"))
summary(max_model)
# save(max_model, file="load/glm_max_model.Rdata")
```

```{r stepwise_selection, include=F, eval=F}

############## STEPWISE
model_stepwise_both <- step(
  base_model, 
  scope = list(upper=max_model, lower=base_model ), 
  direction = "both", 
  trace=T,
  steps=30,
  k=4)
summary(model_stepwise_both)

############ FORWARD
model_stepwise_for<-step(base_model, scope = list(upper=max_model, lower=~1 ), direction = "forward", trace=T,steps=30,k=2)
summary(model_stepwise_for)

########### BACKWARD
model_stepwise_b<-step(max_model,  direction = "backward", trace=T,steps=30,k=2)
summary(model_stepwise_b)

########## SUBSETING - less automated approach
# comparioson of models with defined number of variables
# nbest = number of best models
# nvmax= max muber of variables in the model
library(leaps)
model_nbest<-regsubsets(data[,2:10], data=data, y=data$class, nbest=1, nvmax=15)

plot(model_nbest, scale="adjr2")
plot(model_nbest, scale="bic")



# Summary: I will use the model chosen in the stepwise approach
model <- model_stepwise_both
save(model, file = "load/glm_model.Rdata")

```

```{r model_quality, include=F, eval=F}

# GOF - completely basic
# assumption we compare the obtained model with the "ideal" models and check whether the obtained MLV is statistically close to 0
# H0: the model is well fitted to the data
gf<-pchisq(model$deviance, model$df.residual,lower.tail = F)

# LR test on the significance of variables
# we check if the maxW for the model is significantly larger than for the model only with the constant - test for the total significance of the model
# H0 variables are statistically irrelevant
ist<-pchisq(model$null.deviance-model$deviance, model$df.null-model$df.residual,lower.tail = F)

# Hosmera - Lemeshowa test  - basic GOF test a model with for binary dependent variable
# H0: the model is well fitted to the data
# has many disadvantages - first of all it is very sensitive to the number of buckets
hr<-hosmerlem(y=df$class, yhat=fitted(model),g=10)
hosmerlem(y=df$class, yhat=fitted(model),g=7)
hosmerlem(y=df$class, yhat=fitted(model),g=8)
hosmerlem(y=df$class, yhat=fitted(model),g=9)

df$base_model<-base_model$fitted.values
df$model<-model$fitted.values
df$max_model<-max_model$fitted.values

df$score<-(660-40/log(1/2)*log(1/72))+40/log(1/2)*model$linear.predictors

test$model<-predict(model, newdata=test, type="response") 
test$score<-(660-40/log(1/2)*log(1/72))+40/log(1/2)*predict(model, newdata=test, type="link") 

train$model<-predict(model, newdata=train, type="response") 
train$score<-(660-40/log(1/2)*log(1/72))+40/log(1/2)*predict(model, newdata=train, type="link") 

roc_test_base<-roc.test(df$class, df$model, df$base_model,method="d")$p.value
roc_test_og<-roc.test(df$class, df$max_model, df$model,method="d")$p.value

hist(data[df$class==0,c("score")])
hist(data[df$class==1,c("score")])

gini_t<-2*auc(df$class,df$model,direction="<")-1
gini_w<-2*auc(test$class,test$model,direction="<")-1

ci_delong_t<-2*ci.auc(df$class, df$model,method="d",direction="<")-1
# 0.5205047 0.5366548 0.5528049
ci_delong_w<-2*ci.auc(test$class, test$model,method="d",direction="<")-1

ks_score_t<-ks.test(df[df$class==0,c("score")],df[df$class==1,c("score")])$statistic
ks_score_w<-ks.test(test[test$class==0,c("score")],test[test$class==1,c("score")])$statistic


################################################### stability

psi<-cal_psi(data1=df, data2=test, bench="score",target="score",bin=20)

ks<-ks.test(df$score,test$score)$p.value

t<-as.data.frame(sort(table(df$score)/length(df$score),decreasing=T))[1:3,1:2]
w<-as.data.frame(sort(table(test$score)/length(test$score),decreasing=T))[1:3,1:2]

variables<-names(model$coefficients)[2:length(model$coefficients)]

variables_tab <- data.frame()
models_qual <- data.frame()
var_qual <- data.frame()

for (i in 1:length(variables)) {
  tab<-NULL 
  tab$model<-"model_stepwise_both"
  tab$v<-variables[i]
  tab$gini_t <- 2*ci.auc(
    df[!is.na(df[,variables[i]]),c("class")],
    df[!is.na(df[,variables[i]]),variables[i]],
    direction=">",
    method="d")[2]-1
  tab$gini_w <- 2*ci.auc(
    test[!is.na(test[,variables[i]]),c("class")],
    test[!is.na(test[,variables[i]]),variables[i]],
    direction=">",
    method="d")[2]-1
  
  tab$psi <- cal_psi_zm(
    data1=df[!is.na(df[,variables[i]]),variables], 
    data2=test[!is.na(test[,variables[i]]),variables],
    bench=variables[i],target=variables[i])
  
  tab<-as.data.frame(tab)
  variables_tab<-rbind(variables_tab, tab)
}




temp_tab<-as.data.frame(cbind("Model"="model_stepwise_both",
                              
                              'ist_param'=ist,
                              "roc_test_baza"=roc_test_baza,
                              "hosmer"=hr$p.value,
                              "gf"=gf,
                              "ist_ogr"=ist,
                              "roc_test_og"=roc_test_og,
                              "gini_t_cil"=ci_delong_t[1],
                              "gini_t"=gini_t,
                              "gini_t_ciu"=ci_delong_t[3],
                              "gini_w_cil"=ci_delong_w[1],
                              "gini_w"=gini_w,
                              "gini_w_ciu"=ci_delong_w[3],
                              "ks_score_t"=ks_score_t,
                              "ks_score_w"=ks_score_w,
                              "psi"=psi,
                              "ks_test"=ks,
                              
                              "t_1_n"=t[1,1],
                              "t_1"=t[1,2],
                              "t_2_n"=t[2,1],
                              "t_2"=t[2,2],
                              "t_3_n"=t[3,1],
                              "t_3"=t[3,2],
                              "w_1_n"=w[1,1],
                              "w_1"=w[1,2],
                              "w_2_n"=w[2,1],
                              "w_2"=w[2,2],
                              "w_3_n"=w[3,1],
                              "w_3"=w[3,2]
))


models_qual<-rbind(models_qual,temp_tab)
var_qual<-rbind(var_qual,variables_tab)

save(models_qual,file="models_quality.rdata")
save(var_qual,file="variables_quality.rdata")

models_qual
var_qual

```

## Model specification

Firstly, two models were estimated: a model with constant only (1) and a model with all the variables (2). These models can be benchmarks and used to assess if our chosen model form is correct. To choose the proper functional form, a stepwise selection algorithm was used. Stepwise regression is based on that in each step, a variable is considered for addition or substraction from the set of all variables, based on defined information criterion. We used *stats* package, which uses AIC criterion while number of degrees of freedom used for the penalty is set k = 2. We decided to use more conservative k = 4. 

```{r print_chosen_model, results='asis'}

load("load/glm_model.Rdata")

stargazer(base_model, max_model, model, 
          type = "html",
          single.row = F,
          report = "vc*",
          notes = "<em>&#42;p&lt;0.1;&#42;&#42;p&lt;0.05;&#42;&#42;&#42;p&lt;0.01</em>", 
          notes.append = F)


```

Because we used WoE transformation, we would expect all coefficients to be negative as the higher the WoE, the less probability of default. This could be a problem from the business side, however in this research we focus mainly on forecasts, so we will leave it as it is.

We can also see that Akaike Information Criterion is smaller for the chosen model than for the base and max models. This suggests that our model is a better fit to the data.

## Quality assessment

To assess if the model is specified correctly, we performed tests assessing model quality.

```{r}
# GOF - completely basic
# assumption we compare the obtained model with the "ideal" models and check whether the obtained MLV is statistically close to 0
# H0: the model is well fitted to the data
gf<-pchisq(model$deviance, model$df.residual,lower.tail = F)
```

### Goodness of fit

Firstly, we conducted a Pearson Chi2 and Deviance test. Its null hypothesis says that the model is well fitted to the data. We obtained p-value equal to `r round(gf,4)`, so there is no evidence to reject the null hypothesis.

```{r}
# LR test on the significance of variables
# we check if the maxW for the model is significantly larger than for the model only with the constant - test for the total significance of the model
# H0 variables are statistically irrelevant
ist<-pchisq(model$null.deviance-model$deviance, model$df.null-model$df.residual,lower.tail = F)
```

Then we performed LR test on the joint significance of variables. In this case null hypothesis states that variables are statistically irrelevant. P-value of this test is `r round(ist, 4)`, so we reject the null hypothesis.

```{r}
# Hosmera - Lemeshowa test  - basic GOF test a model with for binary dependent variable
# H0: the model is well fitted to the data
# has many disadvantages - first of all it is very sensitive to the number of buckets
hr_10 <- hosmerlem(y=df$class, yhat=fitted(model),g=10)$p.value
hr_7 <- hosmerlem(y=df$class, yhat=fitted(model),g=7)$p.value
hr_8 <- hosmerlem(y=df$class, yhat=fitted(model),g=8)$p.value
hr_9 <- hosmerlem(y=df$class, yhat=fitted(model),g=9)$p.value
```

Afterwards, we did a Hosmer-Lemeshow test. This test's null hypothesis tells that model is well fitted to the data. As the model is very sensitive to the number of buckets used, we repeated this test for 7, 8, 9 and 10 buckets. The p-values we obtained are respectively `r round(hr_7,4)`, `r round(hr_8,4)`, `r round(hr_9,4)` and `r round(hr_10,4)`. In each case we cannot reject the null hypothesis, which suggests that our formula is correct.

### Discriminatory/predictive power

Next, we tried to assess the discriminatory power of the model. Therefore, we estimated confidence intervals of the Gini index. The rule of the thumb says that for a behavioral model, Gini index should be at least 60%. We can see that in our case, while the training sample achieves this result, the testing sample does not. It might suggest that we have a slight problem with overfitting.

```{r message=F}
train$model<-predict(model, newdata=train, type="response") 
test$model<-predict(model, newdata=test, type="response") 
df$base_model<-base_model$fitted.values
df$model<-model$fitted.values
df$max_model<-max_model$fitted.values

ci_delong_t<-2*ci.auc(df$class, df$model,method="d",direction="<")-1
ci_delong_w<-2*ci.auc(test$class, test$model,method="d",direction="<")-1

conf_int_matrix <- rbind(ci_delong_t, ci_delong_w)
colnames(conf_int_matrix) <- c("Lower limit", "Mean", "Upper limit")
rownames(conf_int_matrix) <- c("Train sample", "Test sample")

kable(conf_int_matrix, digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped", "hover","condensed", "responsive"))

```

```{r message=FALSE, warning=FALSE}
df$score<-(660-40/log(1/2)*log(1/72))+40/log(1/2)*model$linear.predictors
test$score<-(660-40/log(1/2)*log(1/72))+40/log(1/2)*predict(model, newdata=test, type="link") 

ks_score_t<-ks.test(df[df$class==0,c("score")],df[df$class==1,c("score")])$statistic
ks_score_w<-ks.test(test[test$class==0,c("score")],test[test$class==1,c("score")])$statistic

```

We also estimated the K-S statistics. For the test sample it equals `r round(ks_score_t,4)`, while for the train sample `r round(ks_score_w, 4)`. Again we can see that statistics is better for the training sample than for the test sample. The rule of the thumb says that for behavioral model, we should obtain at least 0.5 - we can see we are not that far from this value.

```{r message=FALSE, warning=FALSE}
psi<-cal_psi(data1=df, data2=test, bench="score",target="score",bin=20)

ks<-ks.test(df$score,test$score)$p.value

```

### Stability 

Later we evaluated stability tests, namely PSI and Kolmogorov-Smirnov test. To do that, firstly we calculated score of each customer in the test and train sample. Then, we estimated Population Stability Index (PSI) to compare the distribution of the scoring variable in both sets. The thumb rule states that if PSI is less than 0.1 then no change in scoring model is needed. Our PSI equals to `r round(psi, 4)`, so the model is stable.

The null hypothesis of the Kolmogorov-Smirnov test states that there is no difference between the two distributions. P-value of the test equals to 'r round(ks, 4)` so we have no reason to reject the null hypothesis. 

```{r message=F, warning=FALSE}
roc_test_base<-roc.test(df$class, df$model, df$base_model, method="d")$p.value
roc_test_og<-roc.test(df$class, df$max_model, df$model, method="d")$p.value
```

### Comparison to base and max models

Finally, we compared the difference in areas under the ROC curves with ROC test. The null hypothesis in the test is that the difference is equal to 0, against the alternative hypothesis that it is not equal to 0. When we compare our model to the *base model*, including only intercept, p-value of ROC test is `r round(roc_test_base, 4)`, so the area under curves are different. P-value of ROC test for our model and the *max model*, including all the variables, is `r round(roc_test_og,4)`, so we have no reason to reject the null hypothesis.

### Summary

Taking into consideration all the tests, we decided to use the model obtained by stepwise regression as the final one.

# Random Forests


```{r random_forest_split_train_test, eval=F, include=F}

load("load/df.rda")
load("load/test.RData")
load("load/train.RData")

intersection_test <- intersect(df[,c('age', 'credit_amount', 'duration')], test[, c("age", "credit_amount", "duration")])
intersection_train <- intersect(df[,c('age', 'credit_amount', 'duration')], train[, c("age", "credit_amount", "duration")])

train_raw <- data.frame()
test_raw <- data.frame()

for (i in (1:nrow(intersection_test))){
  age=intersection_test$age[i]
  credit_amount=intersection_test$credit_amount[i]
  duration=intersection_test$duration[i]
  matched_row <- df[df$age %in% age & df$credit_amount %in% credit_amount & df$duration %in% duration,]
  test_raw <- rbind(test_raw,matched_row)
}

for (i in (1:nrow(intersection_train))){
  age=intersection_train$age[i]
  credit_amount=intersection_train$credit_amount[i]
  duration=intersection_train$duration[i]
  matched_row <- df[df$age %in% age & df$credit_amount %in% credit_amount & df$duration %in% duration,]
  train_raw <- rbind(train_raw,matched_row)
}

sum(train_raw$credit_amount)

save(train_raw,file="load/train_raw.RData")
save(test_raw,file="load/test_raw.RData")
```

```{r accuracy_ROC, eval=F, include=F}
library(pROC)
accuracy_ROC <- function(model, 
                         data, 
                         target_variable = "class",
                         predicted_class = "bad") {
  
  # generate probabilities of level "predicted_class"
    forecasts_p <- predict(model, data,
                         type = "prob")[, predicted_class]
  
  # and the predicted  cateogry
  if (any(class(model) == "train")) forecasts_c <- predict(model, data) else
     forecasts_c <- predict(model, data, type = "class")
  
  # actual values - pull() transforms tibble into vector
  real <- (data[, target_variable])
  
  # area under the curve
  AUC <- roc(predictor = forecasts_p,
             response = real)
  
  # classification table and measures based on it 
  table <- confusionMatrix(forecasts_c,
                           real,
                           predicted_class) 
  # collect everything in a single object 
  result <- c(table$overall[1], # Accuracy
              table$byClass[1:2], # sens, spec
              ROC = AUC$auc)
  
  return(result)
  
}
```

```{r random_forest_raw_data, eval=F, include=F}
load("load/test_raw.RData")
load("load/train_raw.RData")

set.seed(1916)
library(randomForest)
library(caret)

model.formula <- class ~ .

# First Random Forest with default parameters
## mtry = sqrt(n) = 4
## ntree = 500
set.seed(1916)
rf1 <- randomForest(model.formula,
                    data = train_raw)
print(rf1)
plot(rf1)

# As the number of trees is higher, the OOB error (black line) is smaller,
# and converges to approx. 400 trees.

# Hence, let us try to limit number of trees and estimate the model
# on bootstrap samples from the full data set.
# We also increase the number of predictors used from 4 to 8.
set.seed(1916)
rf2 <- randomForest(model.formula,
                    data = train_raw,
                    ntree = 400,
                    mtry = 8,
                    # we also generate 
                    # predictors importance measures,
                    importance = TRUE)
print(rf2)

# the OOB error is higher now!
# let us see it again on the plot with respect to number of trees
plot(rf2)

# let us try to optimize the mtry parameter using cross-validation process
# we consider 21 predictors in the model formula hence let us try mtry between 2 and 17
parameters_rf <- expand.grid(mtry = 2:17)

ctrl_cv <- trainControl(method = "cv", 
                         number = 10,
                         classProbs = TRUE)
set.seed(1916)
rf3 <- train(model.formula,
             data = train_raw, 
             method = "rf", 
             ntree = 400,
             tuneGrid = parameters_rf, 
             trControl = ctrl_cv,
             importance = TRUE)

print(rf3)
plot(rf3)

# optimal mtry = 12

set.seed(1916)
rf4 <-  randomForest(model.formula,
                    data = train_raw,
                    ntree = 400,
                    mtry = 12,
                    importance = TRUE)
print(rf4)

save(rf4,file="load/rf4.Rdata")

accuracy_ROC(model = rf1,
             data = train_raw)

accuracy_ROC(model = rf2,
             data = train_raw)

accuracy_ROC(model = rf3,
             data = train_raw)

accuracy_ROC(model = rf4,
             data = train_raw)


# let us compare prediction errors on the testing set 
# from particular models
accuracy_ROC(model = rf1,
             data = test_raw)

accuracy_ROC(model = rf2,
             data = test_raw)

accuracy_ROC(model = rf3,
             data = test_raw)

accuracy_ROC(model = rf4,
             data = test_raw)

# we might have a problem with overfitting, but let it slide

```


Random Forests is a supervised learning method for classification. It is based on generating a large number of decision trees, each constructed using a different subset of the training set. Random forests correct for decision tree's habit of overfitting to the training set.

In case of random forests, we decided to use raw, unprocessed data. Firstly, we set abitrarily the number of trees as ntree=500 and number of predictors as m=4 (number which is the closest to the square root of 20). Then we decided to decrease number of trees to 400, as the OOB error seems to be stable after this point. Afterwards, we used cross-validation process to choose optimal number of m. Finally we decided to use random forest with m=12.

```{r random_forest_print, message=F, error=F}
library(randomForest)
load("load/rf4.Rdata")
print(rf4)
```

Random Forest also generates features' importance plots - mean decrease Accuracy and mean decrease Gini. We can see that the insights from these plots are similar to those based on previous results from Logistic Regression - the most important variables include *checking_status*, *duration* and *credit_amount*. 

```{r random_forest_feature_imp, fig.align='center', fig.asp = .75}
varImpPlot(rf4, main="Features' importance")
```

# Results

```{r calc_results, message=F}

load("load/glm_model.Rdata")
load("load/rf4.Rdata")
load("load/test_final.Rdata")
load("load/test_raw.RData")

# real
real_rf <- test_raw$class
real_glm <- factor(test$class, levels=c(0,1), labels=c("good", "bad"))

# glm
pred_prob_glm <- predict(model, newdata=test, type="response") #  P(Y = 1|X)
pred_class_glm <- factor(ifelse(pred_prob_glm>.5, 1, 0), levels=c(0,1), labels=c("good", "bad"))


# random forest
pred_prob_rf <- predict(rf4, newdata=test_raw, type="prob")[, "bad"]
pred_class_rf <- predict(rf4, newdata=test_raw, type="class")

AUC_glm <- roc(predictor = pred_prob_glm, response = real_glm, plot=F)
AUC_rf <- roc(predictor = pred_prob_rf, response = real_rf, plot=F)

table_glm <- confusionMatrix(pred_class_glm,
                           real_glm,
                           "bad")

table_rf <- confusionMatrix(pred_class_rf,
                           real_rf,
                           "bad")

```

To compare results, firstly we calculated the AuC for both GLM and RF, assuming cut-off point for GLM at 50%. The difference is not big.
AuC for GLM is `r round(AUC_glm$auc,4)`, while for the RF it is `r round(AUC_rf$auc,4)`.

Next, we plotted the confusion matrix for both models.

```{r plot_confusion_matrix, message=F, fig.align='center', fig.asp = .7, warning=F}
source("plot_confusion_matrix.R")
draw_confusion_matrix(table_glm,mytitle="Confustion matrix for Logistic Regression")
draw_confusion_matrix(table_rf,mytitle="Confustion matrix for Random Forest")
```

Both the Accuracy and the Sensivity are better for Random Forest, thus it seems that Random Forest outperforms Logistic Regression.

# Summary

In this research I compared predictive power of Logistic Regression and Random Forest in the context of Credit Scorecard. Firstly I estimated GLM model, in accordance with best practices and subsequently I trained a Random Forest, using cross-validation to optimise hyperparameters. Random Forest seems to outperform Logistic Regression, however the difference is not immense. In case of Credit Scorecard building, GLM models still are preferable as they provide clear answer about how each trait of a client contributes to their Credit Score.
